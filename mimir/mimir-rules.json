{
    "status": "success",
    "data": {
      "groups": [
        {
          "name": "Node Alerts",
          "file": "Mimir",
          "rules": [
            {
              "state": "inactive",
              "name": "Node Down Alert",
              "query": "up{job=\"kubernetes-nodes\"} == 0",
              "duration": 60,
              "keepFiringFor": 0,
              "labels": {
                "severity": "critical"
              },
              "annotations": {
                "description": "The node {{ $labels.instance }} running node-exporter has been unreachable for more than 5 minutes.\n\nThis could indicate a crash, network failure, or power issue.\n\nPlease investigate and restore the node as soon as possible.",
                "summary": "\"üö® Node {{ $labels.instance }} is down!\""
              },
              "alerts": [],
              "health": "ok",
              "lastError": "",
              "type": "alerting",
              "lastEvaluation": "2025-02-02T09:07:12.053305329Z",
              "evaluationTime": 0.002526298
            }
          ],
          "interval": 60,
          "lastEvaluation": "2025-02-02T09:07:12.053198537Z",
          "evaluationTime": 0.002641371,
          "sourceTenants": null
        },
        {
          "name": "PVC Alerts",
          "file": "Mimir",
          "rules": [
            {
              "state": "inactive",
              "name": "PVC Pending Alert",
              "query": "kube_persistentvolumeclaim_status_phase{phase=\"Pending\"} > 0",
              "duration": 60,
              "keepFiringFor": 0,
              "labels": {
                "severity": "high"
              },
              "annotations": {
                "description": "PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} has been in Pending state for over 10 minutes.\n\nPossible reasons:\n            - No available storage class\n            - Insufficient disk space\n            - Storage provider issues\n\nRun `kubectl describe pvc {{ $labels.persistentvolumeclaim }}` to troubleshoot.",
                "summary": "\"üõë PVC {{ $labels.persistentvolumeclaim }} is stuck in Pending!\""
              },
              "alerts": [],
              "health": "ok",
              "lastError": "",
              "type": "alerting",
              "lastEvaluation": "2025-02-02T09:07:01.89100332Z",
              "evaluationTime": 0.003247646
            },
            {
              "state": "inactive",
              "name": "PVC Lost Alert",
              "query": "kube_persistentvolumeclaim_status_phase{phase=\"Lost\"} > 0",
              "duration": 60,
              "keepFiringFor": 0,
              "labels": {
                "severity": "low"
              },
              "annotations": {
                "description": "PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} has been in Lost state for over 5 minutes.\n\nThis means the persistent volume backing this claim is no longer accessible.\n            Possible reasons:\n            - Underlying storage failure\n            - Node failure\n            - Network disruption\n\nCheck storage logs and PV status.",
                "summary": "\"üö® PVC {{ $labels.persistentvolumeclaim }} is LOST!\""
              },
              "alerts": [],
              "health": "ok",
              "lastError": "",
              "type": "alerting",
              "lastEvaluation": "2025-02-02T09:07:01.89427369Z",
              "evaluationTime": 0.002358362
            },
            {
              "state": "inactive",
              "name": "PVC Failed Alert",
              "query": "kube_persistentvolumeclaim_status_phase{phase=\"Failed\"} > 0",
              "duration": 60,
              "keepFiringFor": 0,
              "labels": {
                "severity": "high"
              },
              "annotations": {
                "description": "PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} has failed.\n\nThis could indicate a major issue with the storage backend.\n\nCheck `kubectl get pv` and `kubectl describe pvc {{ $labels.persistentvolumeclaim }}`.",
                "summary": "\"üí• PVC {{ $labels.persistentvolumeclaim }} has FAILED!\""
              },
              "alerts": [],
              "health": "ok",
              "lastError": "",
              "type": "alerting",
              "lastEvaluation": "2025-02-02T09:07:01.896643233Z",
              "evaluationTime": 0.001154888
            }
          ],
          "interval": 60,
          "lastEvaluation": "2025-02-02T09:07:01.89096042Z",
          "evaluationTime": 0.006841909,
          "sourceTenants": null
        },
        {
          "name": "Pod Alerts",
          "file": "Mimir",
          "rules": [
            {
              "state": "inactive",
              "name": "Pod Pending Alert",
              "query": "kube_pod_status_phase{phase=\"Pending\"} > 0",
              "duration": 60,
              "keepFiringFor": 0,
              "labels": {
                "severity": "normal"
              },
              "annotations": {
                "description": "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been Pending for over 10 minutes.\nPossible reasons:\n            - Insufficient resources (CPU, memory)\n            - No available nodes\n            - Image pull failure\nInvestigate by checking `kubectl describe pod {{ $labels.pod }}`.",
                "summary": "\"‚è≥ Pod {{ $labels.pod }} is stuck in Pending!\""
              },
              "alerts": [],
              "health": "ok",
              "lastError": "",
              "type": "alerting",
              "lastEvaluation": "2025-02-02T09:06:47.779709962Z",
              "evaluationTime": 0.004900152
            },
            {
              "state": "inactive",
              "name": "Pod Failed Alert",
              "query": "kube_pod_status_phase{phase=\"Failed\"} > 0",
              "duration": 60,
              "keepFiringFor": 0,
              "labels": {
                "severity": "normal"
              },
              "annotations": {
                "description": "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has failed.\n\nPossible reasons:\n            - Application crash\n            - Out-of-memory (OOMKill)\n            - Node failure\n\nCheck logs using `kubectl logs {{ $labels.pod }}`.",
                "summary": "\"üí• Pod {{ $labels.pod }} has failed!\""
              },
              "alerts": [],
              "health": "ok",
              "lastError": "",
              "type": "alerting",
              "lastEvaluation": "2025-02-02T09:06:47.784622346Z",
              "evaluationTime": 0.004427976
            },
            {
              "state": "inactive",
              "name": "Pod Crash Loop Backoff Alert",
              "query": "kube_pod_container_status_waiting{reason=\"CrashLoopBackOff\"} > 0",
              "duration": 60,
              "keepFiringFor": 0,
              "labels": {
                "severity": "high"
              },
              "annotations": {
                "description": "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been crashing repeatedly for over 5 minutes.\nPossible reasons:\n            - Application misconfiguration\n            - Missing dependencies\n            - Resource exhaustion\nRun `kubectl describe pod {{ $labels.pod }}` to debug.",
                "summary": "\"üîÅ Pod {{ $labels.pod }} is in CrashLoopBackOff!\""
              },
              "alerts": [],
              "health": "ok",
              "lastError": "",
              "type": "alerting",
              "lastEvaluation": "2025-02-02T09:06:47.789063868Z",
              "evaluationTime": 0.00141051
            },
            {
              "state": "inactive",
              "name": "Pod Unknown Alert",
              "query": "kube_pod_status_phase{phase=\"Unknown\"} > 0",
              "duration": 60,
              "keepFiringFor": 0,
              "labels": {
                "severity": "low"
              },
              "annotations": {
                "description": "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been in Unknown state for over 5 minutes.\n\nThis usually happens when:\n            - The kubelet loses connection to the pod\n            - The node hosting the pod is unreachable\n\nCheck `kubectl get pods -o wide` for more details.",
                "summary": "\"‚ùì Pod {{ $labels.pod }} is in Unknown state!\""
              },
              "alerts": [],
              "health": "ok",
              "lastError": "",
              "type": "alerting",
              "lastEvaluation": "2025-02-02T09:06:47.790484696Z",
              "evaluationTime": 0.003771713
            }
          ],
          "interval": 60,
          "lastEvaluation": "2025-02-02T09:06:47.779675368Z",
          "evaluationTime": 0.014586242,
          "sourceTenants": null
        },
        {
          "name": "Service",
          "file": "Mimir",
          "rules": [
            {
              "state": "inactive",
              "name": "Spring Service Down",
              "query": "up{job=\"spring-services\"} == 0",
              "duration": 60,
              "keepFiringFor": 0,
              "labels": {
                "severity": "high"
              },
              "annotations": {
                "description": "The service instance {{ $labels.instance }} in namespace {{ $labels.namespace }} has been unresponsive for over 3 minutes.\n\nThis could be due to a pod crash, networking issues, or deployment failure.\n\nCheck pod logs and service status.",
                "summary": "\"üî• Spring Service {{ $labels.instance }} is down!\""
              },
              "alerts": [],
              "health": "ok",
              "lastError": "",
              "type": "alerting",
              "lastEvaluation": "2025-02-02T09:07:36.347128171Z",
              "evaluationTime": 0.002290153
            }
          ],
          "interval": 60,
          "lastEvaluation": "2025-02-02T09:07:36.347071064Z",
          "evaluationTime": 0.002352782,
          "sourceTenants": null
        }
      ]
    },
    "errorType": "",
    "error": ""
  }